{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "layout = {\n",
    "    'showlegend': True,\n",
    "    'margin': {'b':10, 'l':20, 'r':50, 't':50},\n",
    "    'font': {'size': 19},\n",
    "    'xaxis': {'zerolinewidth': 2, 'zerolinecolor':'black'},\n",
    "    'yaxis': {'zerolinewidth': 2, 'zerolinecolor':'black'},\n",
    "    'template': 'plotly_white',\n",
    "}\n",
    "\n",
    "px.defaults.color_discrete_sequence = px.colors.qualitative.T10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Quoilin dataset\n",
    "Data available at: https://github.com/squoilin/Self-Consumption/releases Synthetic.Household.Profiles.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_individuals = pd.read_pickle(\"../2_chapter/fr_quoilin_data_valence.pickle\")\n",
    "individuals = _individuals.copy()\n",
    "house_ids = individuals.columns.difference([\"pv_1kw\"])\n",
    "\n",
    "# Fix variables\n",
    "deltat = timedelta(minutes=15)\n",
    "\n",
    "# Training = 31 days + 1 day for lagged values\n",
    "training = timedelta(days=31)\n",
    "\n",
    "# Where do we forecast?\n",
    "start = datetime(2019, 5, 30, 6, 0, 0)\n",
    "full_horizon = timedelta(days=7)\n",
    "\n",
    "# When do we calibrate\n",
    "start_calibrate = start - timedelta(days=2)\n",
    "end_calibrate = start - deltat\n",
    "start_training_to_calibrate = start_calibrate - training\n",
    "end_training_to_calibrate = start_calibrate - deltat\n",
    "\n",
    "# To truncate data\n",
    "end = start + full_horizon + timedelta(days=7)  # keep a few days after anyway\n",
    "\n",
    "pv_size = 3\n",
    "individuals = individuals.loc[start_training_to_calibrate-timedelta(days=1):end, :].copy()\n",
    "for col in individuals.columns:\n",
    "    individuals[col] -= pv_size * individuals[\"pv_1kw\"]\n",
    "individuals.drop(columns=[\"pv_1kw\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GAM():\n",
    "    \"\"\"Generalized Additive Model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output, regressors=None,\n",
    "                 daily_seasonality=\"auto\",\n",
    "                 seasonality_prior_scale=10.0):\n",
    "\n",
    "        self._output = output\n",
    "        self._model = Prophet(\n",
    "            growth='flat',\n",
    "            yearly_seasonality=False,\n",
    "            weekly_seasonality=\"auto\",\n",
    "            daily_seasonality=daily_seasonality,\n",
    "            seasonality_mode=\"additive\",\n",
    "            interval_width=0.95,\n",
    "            changepoint_prior_scale=0.05,\n",
    "            seasonality_prior_scale=seasonality_prior_scale,\n",
    "            uncertainty_samples=False,\n",
    "        )\n",
    "        if regressors is None:\n",
    "            regressors = list()\n",
    "        for reg in regressors:\n",
    "            self._model.add_regressor(\n",
    "                name=reg[\"name\"],\n",
    "                prior_scale=reg[\"prior_scale\"])\n",
    "\n",
    "    def fit(self, df):\n",
    "        with suppress_stdout_stderr():\n",
    "            self._model.fit(self._specific_formatting(df))\n",
    "\n",
    "    def predict(self, df):\n",
    "        forecast = self._model.predict(self._specific_formatting(df))\n",
    "        forecast.set_index(\"ds\", inplace=True, drop=True)\n",
    "        forecast.drop(columns=forecast.columns.difference([\"yhat\"]), inplace=True)\n",
    "        forecast.rename(columns={\"yhat\": self._output}, inplace=True)\n",
    "        return forecast\n",
    "\n",
    "    def _specific_formatting(self, df):\n",
    "        df = df.copy()\n",
    "        df[\"ds\"] = df.index.tz_localize(None)\n",
    "        df.rename(columns={self._output: \"y\"}, inplace=True)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "        return df\n",
    "\n",
    "\n",
    "class suppress_stdout_stderr(object):\n",
    "    \"\"\"\n",
    "    A context manager for doing a \"deep suppression\" of stdout and stderr in\n",
    "    Python, i.e. will suppress all print, even if the print originates in a\n",
    "    compiled C/Fortran sub-function.\n",
    "       This will not suppress raised exceptions, since exceptions are printed\n",
    "    to stderr just before a script exits, and after the context manager has\n",
    "    exited (at least, I think that is why it lets exceptions through).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n",
    "        self.save_fds = [os.dup(1), os.dup(2)]\n",
    "\n",
    "    def __enter__(self):\n",
    "        os.dup2(self.null_fds[0], 1)\n",
    "        os.dup2(self.null_fds[1], 2)\n",
    "\n",
    "    def __exit__(self, *_):\n",
    "        os.dup2(self.save_fds[0], 1)\n",
    "        os.dup2(self.save_fds[1], 2)\n",
    "        for fd in self.null_fds + self.save_fds:\n",
    "            os.close(fd)\n",
    "\n",
    "\n",
    "def get_gof(df, result, ref_col, pred_col):\n",
    "    \"\"\"\n",
    "    gof = (1 - NRMSE) * 100\n",
    "    \"\"\"\n",
    "    pred = result.loc[:, [pred_col]].copy()\n",
    "    #if pred.index.tzinfo is None:\n",
    "    #    pred.index = pred.index.tz_localize(\"UTC\")\n",
    "    pred.columns = [\"prediction\"]\n",
    "\n",
    "    ref = df.loc[pred.index[0]:pred.index[-1], [ref_col]].copy()\n",
    "    ref.columns = [\"target\"]\n",
    "\n",
    "    nrmse = (np.linalg.norm(ref[\"target\"].values - pred[\"prediction\"].values, 2)\n",
    "           / np.linalg.norm(ref[\"target\"].values - ref[\"target\"].mean(), 2))\n",
    "    return 100.0 * (1.0 - np.clip(nrmse, a_min=0.0, a_max=1.0))\n",
    "\n",
    "def lag_values(df, nb_lag, output_col):\n",
    "    tmp = df.copy()\n",
    "    for shift in nb_lag:\n",
    "        tmp[f\"t-{shift}\"] = tmp[output_col].shift(shift)\n",
    "    return tmp\n",
    "\n",
    "def predict_n_periods_with_autoreg(df, start_training, end_training, horizon,\n",
    "                                   deltat, end_complete_pred, freq, output_col,\n",
    "                                   regressors, nb_lag, seasonality_prior_scale=1.0,\n",
    "                                   disable_progress_bar=False, daily_seasonality=\"auto\"):\n",
    "    \"\"\"\n",
    "    Train a GAM and predict for horizon T\n",
    "    Shift prediction start and predict over T again.\n",
    "\n",
    "    results : [pd.DataFrame] One frame per prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = [reg[\"name\"] for reg in regressors]\n",
    "    results = []\n",
    "    model = GAM(output_col, regressors, daily_seasonality=daily_seasonality,\n",
    "                seasonality_prior_scale=seasonality_prior_scale)\n",
    "\n",
    "    tmp = lag_values(df.loc[start_training-timedelta(days=1):end_training], nb_lag, output_col)\n",
    "    model.fit(tmp.loc[start_training:end_training])\n",
    "\n",
    "    # Number of forecast where we have access to actual data\n",
    "    forecast_freq = pd.date_range(end_training + deltat, end_complete_pred, freq=freq)\n",
    "    for start_prediction in tqdm(forecast_freq, desc=\"# Forecast: \", disable=disable_progress_bar):\n",
    "        tmp_results = []\n",
    "        end_prediction = start_prediction + horizon\n",
    "\n",
    "        # Get lagged values and NaN to blank future info\n",
    "        tmp = lag_values(df.loc[start_prediction-timedelta(days=1):end_prediction], nb_lag, output_col)\n",
    "        tmp = tmp.loc[start_prediction:end_prediction]\n",
    "        for n in nb_lag:\n",
    "            tmp.loc[:, f\"t-{n}\"] = tmp[f\"t-{n}\"].iloc[0:n].tolist() + ([np.nan] * (len(tmp) - n))\n",
    "\n",
    "        horizon_spam = pd.date_range(start_prediction, end_prediction, freq=\"15T\")\n",
    "        for step_i, step in enumerate(horizon_spam):\n",
    "            # Fill up NaN of lagged values with previous results\n",
    "            for n in nb_lag:\n",
    "                if pd.isna(tmp.at[step, f\"t-{n}\"]):\n",
    "                    tmp.at[step, f\"t-{n}\"] = tmp_results[step_i-n]\n",
    "\n",
    "            res = model.predict(tmp.loc[step:step, inputs])\n",
    "            tmp_results.append(res.at[step, output_col])\n",
    "\n",
    "        results.append(pd.DataFrame(index=horizon_spam, data={output_col: tmp_results}))\n",
    "    return results, model\n",
    "\n",
    "def model_3(graph, start_training, end_training, horizon,\n",
    "            deltat, end_complete_pred, freq, output_col, scenario):\n",
    "\n",
    "    regressors = [{\"name\": \"pv_1kw\", \"prior_scale\": scenario[\"PRIOR_GHI\"]}]\n",
    "\n",
    "    for hour in range(0, 24):\n",
    "        regressors.append({\"name\": f\"h{hour}\", \"prior_scale\": scenario[\"PRIOR_HOUR\"]})\n",
    "\n",
    "    nb_lag = list(range(1, scenario[\"NB_LAG\"] + 1))\n",
    "    for n in nb_lag:\n",
    "        regressors.append({\"name\": f\"t-{n}\", \"prior_scale\": scenario[\"PRIOR_LAG\"]})\n",
    "\n",
    "    results, _ = predict_n_periods_with_autoreg(\n",
    "        graph, start_training, end_training, horizon, deltat, end_complete_pred, freq, output_col,\n",
    "        regressors,\n",
    "        nb_lag=nb_lag,\n",
    "        seasonality_prior_scale=scenario[\"PRIOR_SEASON\"],\n",
    "        daily_seasonality=scenario[\"DAILY_FOURIER\"],\n",
    "        disable_progress_bar=True)\n",
    "    return results\n",
    "\n",
    "def calibrate(df, individuals, start_training, end_training, horizon, deltat, end_complete_pred, freq, output_col):\n",
    "    gofs = []\n",
    "    reference = {\n",
    "         \"NB_LAG\": 4,\n",
    "         \"PRIOR_GHI\": 3.0,\n",
    "         \"PRIOR_LAG\": 10.0,\n",
    "         \"PRIOR_HOUR\": 10.0,\n",
    "         \"PRIOR_SEASON\": 1.0,\n",
    "         \"DAILY_FOURIER\": \"auto\"}\n",
    "\n",
    "    scenarios = [reference]\n",
    "    for i in range(1, 14 + 1):  #  range(1, 24 + 1)\n",
    "        scenarios.append(dict(reference))\n",
    "        scenarios[-1][\"NB_LAG\"] = i\n",
    "\n",
    "    for i in [1, 5, 8, 10, 15, 20]:\n",
    "        scenarios.append(dict(reference))\n",
    "        scenarios[-1][\"PRIOR_GHI\"] = i\n",
    "\n",
    "    for i in [1, 5, 15]:  #  [1, 3, 5, 8, 15, 20]\n",
    "        scenarios.append(dict(reference))\n",
    "        scenarios[-1][\"PRIOR_LAG\"] = i\n",
    "\n",
    "    for i in [8, 15]:  #  [1, 3, 5, 8, 15, 20]\n",
    "        scenarios.append(dict(reference))\n",
    "        scenarios[-1][\"PRIOR_HOUR\"] = i\n",
    "\n",
    "    for i in [3]:  #  [3, 5, 8, 10, 15, 20]\n",
    "        scenarios.append(dict(reference))\n",
    "        scenarios[-1][\"PRIOR_SEASON\"] = i\n",
    "\n",
    "    #for i in [\"auto\", 5, 10, 15, 20, 30]:  # no test\n",
    "    #    scenarios.append(dict(reference))\n",
    "    #    scenarios[-1][\"DAILY_FOURIER\"] = i\n",
    "\n",
    "    graph = individuals[[output_col]].copy()\n",
    "    graph[\"pv_1kw\"] = df.loc[graph.index[0]:graph.index[-1], \"pv_1kw\"]\n",
    "\n",
    "    graph[\"_datetime\"] = graph.index\n",
    "    for hour in range(0, 24):\n",
    "        graph[f\"h{hour}\"] = graph._datetime.apply(lambda x: 1.0 if x.hour == hour else 0)\n",
    "    graph.drop(columns=\"_datetime\", inplace=True)\n",
    "\n",
    "    for scenario in tqdm(scenarios, desc=\"Calibration :\"):\n",
    "        results = model_3(graph, start_training, end_training, horizon,\n",
    "                    deltat, end_complete_pred, freq, output_col, scenario)\n",
    "\n",
    "        results = pd.concat(results, axis=0)\n",
    "        gofs.append(get_gof(individuals, results, output_col, output_col))\n",
    "        #tmp_gof = []\n",
    "        #for result in results:\n",
    "        #    tmp_gof.append(get_gof(individuals, result, output_col, output_col))\n",
    "        #gofs.append(np.mean(tmp_gof))\n",
    "\n",
    "    results = pd.DataFrame(data=scenarios)\n",
    "    results[\"gof\"] = gofs\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrate & Forecast for each house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_calibration = {\n",
    "                 \"NB_LAG\": 10,\n",
    "                 \"PRIOR_GHI\": 10.0,\n",
    "                 \"PRIOR_LAG\": 10.0,\n",
    "                 \"PRIOR_HOUR\": 10.0,\n",
    "                 \"PRIOR_SEASON\": 1.0,\n",
    "                 \"DAILY_FOURIER\": \"auto\"    \n",
    "}\n",
    "best_parameters = {name:dict(default_calibration) for name in house_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_house = 170\n",
    "\n",
    "start_training = start - training\n",
    "end_training = start - deltat\n",
    "\n",
    "horizon = timedelta(days=2) - deltat\n",
    "end_complete_pred = start + timedelta(days=7)\n",
    "\n",
    "freq = \"2D\"\n",
    "freq_delta = timedelta(days=2) - deltat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "House #: 100%|██████████| 70/70 [1:22:27<00:00, 70.68s/it]\n"
     ]
    }
   ],
   "source": [
    "house_netload = {}\n",
    "#for house_id in tqdm(house_ids[0:nb_house], desc=\"House #\"):\n",
    "\n",
    "for house_id in tqdm(house_ids[nb_house:nb_house+70], desc=\"House #\"):\n",
    "    # Add regressor data\n",
    "    graph = individuals[[house_id]].copy()\n",
    "    graph[\"pv_1kw\"] = _individuals.loc[graph.index[0]:graph.index[-1], \"pv_1kw\"]\n",
    "\n",
    "    graph[\"_datetime\"] = graph.index\n",
    "    for hour in range(0, 24):\n",
    "        graph[f\"h{hour}\"] = graph._datetime.apply(lambda x: 1.0 if x.hour == hour else 0)\n",
    "    graph.drop(columns=\"_datetime\", inplace=True)\n",
    "    \n",
    "    # Run model 3\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "        results = model_3(graph, start_training, end_training, horizon,\n",
    "                    deltat, end_complete_pred, freq, house_id, best_parameters[house_id])\n",
    "    \n",
    "    house_netload[house_id] = []\n",
    "    for i in range(0, len(results)):\n",
    "        _start = results[i].index[0]\n",
    "        house_netload[house_id].append(results[i].loc[_start:_start + freq_delta, :].copy())\n",
    "    house_netload[house_id] = pd.concat(house_netload[house_id], axis=0)\n",
    "        \n",
    "with open(f'tmp_forecasts/70_houses_forecast_2D.pickle', 'wb') as handle:\n",
    "    pickle.dump(house_netload, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly pack households in communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'tmp_forecasts/70_houses_forecast_2D.pickle', 'rb') as handle:\n",
    "    house70 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'tmp_forecasts/100_houses_forecast_2D.pickle', 'rb') as handle:\n",
    "    house100 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_netload = dict(house100, **house70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "community_netload = {}\n",
    "community_sizes = np.linspace(2, 120, 119, dtype=int)\n",
    "\n",
    "for size in community_sizes:\n",
    "    random_ids = random.sample(list(house_ids[0:nb_house]), k=size)\n",
    "    concats = [house_netload[i] for i in random_ids]\n",
    "    community_netload[size] = pd.concat(concats, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_bill(df, member_ids, timing, deltat=15, buy_price=0.1740, exchange_price=0.0371):\n",
    "    \"\"\"Return cost for the community\n",
    "    cost = buy [€/kWh] imports + pay fees on exchanges [€]\n",
    "    \"\"\"\n",
    "    # Community import (sum then max)\n",
    "    pos_netload = (\n",
    "        (\n",
    "            df[[a for a in member_ids]].groupby(pd.Grouper(freq=timing)).sum().sum(axis=1)\n",
    "            #- df[[prod_col for a in member_ids]].groupby(pd.Grouper(freq=timing)).sum().sum(axis=1)\n",
    "        )\n",
    "        .clip(lower=0)\n",
    "        .sum()\n",
    "        * deltat\n",
    "        / 60\n",
    "    )\n",
    "\n",
    "    # Sum of ind import (max then sum)\n",
    "    pos_n_netload = (\n",
    "        sum(\n",
    "            (df[a]).groupby(pd.Grouper(freq=timing)).sum().clip(lower=0).sum()\n",
    "            #(df[a] - df[prod_col]).groupby(pd.Grouper(freq=timing)).sum().clip(lower=0).sum()\n",
    "            for a in member_ids\n",
    "        )\n",
    "        * deltat\n",
    "        / 60\n",
    "    )\n",
    "\n",
    "    return np.round(\n",
    "        buy_price * pos_netload + exchange_price * (pos_n_netload - pos_netload), 3\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_start = end_training + deltat\n",
    "_end = end_complete_pred\n",
    "\n",
    "cost_pvonly = []\n",
    "\n",
    "for community_size in community_sizes:\n",
    "    sub_house_ids = list(community_netload[community_size].columns)\n",
    "    graph = individuals.loc[_start:_end, sub_house_ids].copy()\n",
    "\n",
    "    forecast_cost = community_bill(graph, sub_house_ids, timing=\"15T\", deltat=15, buy_price=0.1740, exchange_price=0.0371)\n",
    "    cost_pvonly.append(forecast_cost / community_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = pd.DataFrame(index=community_sizes, \n",
    "                     data={\"PV_only\": cost_pvonly})\n",
    "#graph = graph[graph[\"PV_only\"] < 13]\n",
    "fig = px.line(graph)\n",
    "\n",
    "fig.update_traces(line_width=3)\n",
    "fig.update_layout(\n",
    "    layout,\n",
    "    height=500,\n",
    "    width=800,\n",
    "    yaxis_title=\"Cost [€/member]\",\n",
    "    xaxis_title=\"Community sizes [kWh]\",\n",
    "    yaxis_range=[0, 15],\n",
    "    xaxis_range=[0, 120],\n",
    "    yaxis_dtick=1,\n",
    "    yaxis_showline=False, yaxis_linewidth=2, yaxis_linecolor='black',\n",
    "    xaxis_showline=False, xaxis_linewidth=2, xaxis_linecolor='black',\n",
    "    showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyomo.opt import SolverFactory\n",
    "from pyomo.environ import *\n",
    "from dataclasses import dataclass\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OptimizeBatteries_quadratic:\n",
    "    grid_buy: float\n",
    "    grid_fee: float\n",
    "    battery_aging_cost: float = 0.0\n",
    "    only_return_schedules: bool = True\n",
    "    freq: int = 15\n",
    "    optim_horizon: timedelta = timedelta(days=2)\n",
    "    solver: str = \"gurobi\"\n",
    "    solver_path: str = None\n",
    "    verbose: bool = False\n",
    "    \n",
    "    def _solve(self, netloads, members, assets):\n",
    "        m = ConcreteModel()\n",
    "        batteries = assets[\"batt\"]\n",
    "\n",
    "        m.horizon = Set(initialize=list(netloads.keys()), ordered=True)\n",
    "        m.members = Set(initialize=list(members.keys()), ordered=True)\n",
    "        m.batteries = Set(initialize=list(batteries.keys()), ordered=True)\n",
    "\n",
    "        m.community_import = Var(m.horizon, domain=NonNegativeReals)\n",
    "        m.member_import = Var(m.horizon, m.members, domain=NonNegativeReals)\n",
    "        m.batteryin = Var(m.horizon, m.batteries, domain=NonNegativeReals)\n",
    "        m.batteryout = Var(m.horizon, m.batteries, domain=NonNegativeReals)\n",
    "        m.batteryenergy = Var(m.horizon, m.batteries, domain=NonNegativeReals)\n",
    "\n",
    "        m.grid_buy = Param(initialize=self.grid_buy)\n",
    "        m.grid_fee = Param(initialize=self.grid_fee)\n",
    "        m.battery_aging_cost = Param(initialize=self.battery_aging_cost)\n",
    "        m.deltat = Param(initialize=self.freq / 60)\n",
    "        m.last = Param(initialize=m.horizon.last())\n",
    "\n",
    "        # Battery constraints\n",
    "        def r_battery_max_powerin(m, t, b):\n",
    "            return m.batteryin[t, b] <= batteries[b][\"max_kw\"]\n",
    "\n",
    "        def r_battery_max_powerout(m, t, b):\n",
    "            return m.batteryout[t, b] <= batteries[b][\"min_kw\"]\n",
    "\n",
    "        def r_battery_energy(m, t, b):\n",
    "            if t == 0:\n",
    "                return m.batteryenergy[t, b] == batteries[b][\"init_kwh\"]\n",
    "            else:\n",
    "                return (\n",
    "                    m.batteryenergy[t, b]\n",
    "                    == m.batteryenergy[t - 1, b]\n",
    "                    + m.batteryin[t - 1, b] * m.deltat * batteries[b][\"eta\"]\n",
    "                    - m.batteryout[t - 1, b] * m.deltat / batteries[b][\"eta\"]\n",
    "                )\n",
    "\n",
    "        def r_battery_min_energy(m, t, b):\n",
    "            return (\n",
    "                m.batteryenergy[t, b]\n",
    "                >= batteries[b][\"max_kwh\"] * batteries[b][\"offset\"]\n",
    "            )\n",
    "\n",
    "        def r_battery_max_energy(m, t, b):\n",
    "            return m.batteryenergy[t, b] <= batteries[b][\"max_kwh\"] * (\n",
    "                1 - batteries[b][\"offset\"]\n",
    "            )\n",
    "\n",
    "        def r_battery_end_power_out(m, b):\n",
    "            return m.batteryout[m.last, b] == 0.0\n",
    "\n",
    "        def r_battery_end_power_in(m, b):\n",
    "            return m.batteryin[m.last, b] == 0.0\n",
    "\n",
    "        # Energy balance\n",
    "        def r_community_import(m, t):\n",
    "            return m.community_import[t] >= sum(\n",
    "                m.batteryin[t, b] - m.batteryout[t, b] for b in m.batteries\n",
    "            ) + sum(netloads[t][p] for p in m.members)\n",
    "\n",
    "        def r_member_import(m, t, p):\n",
    "            return (\n",
    "                m.member_import[t, p]\n",
    "                >= sum(\n",
    "                    m.batteryin[t, b] - m.batteryout[t, b] for b in members[p][\"batt\"]\n",
    "                )\n",
    "                + netloads[t][p]\n",
    "            )\n",
    "        \n",
    "        m.r1 = Constraint(m.horizon, m.batteries, rule=r_battery_max_powerin)\n",
    "        m.r2 = Constraint(m.horizon, m.batteries, rule=r_battery_max_powerout)\n",
    "        m.r3 = Constraint(m.horizon, m.batteries, rule=r_battery_energy)\n",
    "        m.r4 = Constraint(m.horizon, m.batteries, rule=r_battery_min_energy)\n",
    "        m.r5 = Constraint(m.horizon, m.batteries, rule=r_battery_max_energy)\n",
    "        m.r7 = Constraint(m.batteries, rule=r_battery_end_power_out)\n",
    "        m.r8 = Constraint(m.batteries, rule=r_battery_end_power_in)\n",
    "        m.r9 = Constraint(m.horizon, rule=r_community_import)\n",
    "        m.r10 = Constraint(m.horizon, m.members, rule=r_member_import)\n",
    "\n",
    "        def objective_function_bis(m):\n",
    "            return sum(\n",
    "                m.grid_buy * m.community_import[i]\n",
    "                + m.grid_fee\n",
    "                * (\n",
    "                    sum(m.member_import[i, p] for p in m.members)\n",
    "                    - m.community_import[i]\n",
    "                )\n",
    "                + m.battery_aging_cost * sum((1/netloads[i][p] * (m.batteryin[i, b] + m.batteryout[i, b]))**2 for b, p in zip(m.batteries, m.members))\n",
    "                for i in m.horizon\n",
    "            )\n",
    "     \n",
    "        def objective_function(m):\n",
    "            return sum(\n",
    "                m.grid_buy * m.community_import[i]\n",
    "                + m.grid_fee\n",
    "                * (\n",
    "                    sum(m.member_import[i, p] for p in m.members)\n",
    "                    - m.community_import[i]\n",
    "                )\n",
    "                + m.battery_aging_cost * sum((m.batteryin[i, b] + m.batteryout[i, b])**2 for b in m.batteries)\n",
    "                for i in m.horizon\n",
    "            )\n",
    "        m.objective = Objective(rule=objective_function_bis, sense=minimize)\n",
    "\n",
    "        with SolverFactory(self.solver, executable=self.solver_path) as opt:\n",
    "            results = opt.solve(m, tee=False)\n",
    "            if self.verbose:\n",
    "                print(results)\n",
    "        return m\n",
    "   \n",
    "    def solve(self, df, member_ids, spec):\n",
    "        netloads = df.copy()\n",
    "        netloads.index = range(0, len(netloads))\n",
    "        netloads = netloads.T.to_dict()\n",
    "        \n",
    "        members = {member_id:{\"batt\": [int(i)]} for i, member_id in enumerate(member_ids)}\n",
    "        \n",
    "        assets = {}\n",
    "        assets[\"batt\"] = {i:{\n",
    "            \"min_kw\": spec[i][\"min_kw\"],\n",
    "            \"max_kw\": spec[i][\"max_kw\"],\n",
    "            \"max_kwh\": spec[i][\"max_kwh\"],\n",
    "            \"eta\": spec[i][\"eta\"],\n",
    "            \"offset\": spec[i][\"offset\"],\n",
    "            \"init_kwh\": spec[i][\"init_kwh\"],\n",
    "            #\"end_kwh\": spec[\"max_kwh\"] * spec[\"offset\"]\n",
    "        } for i, member_id in enumerate(member_ids)}\n",
    "\n",
    "        model = self._solve(netloads, members, assets)\n",
    "        return self.postprocess(model, df, member_ids)\n",
    "\n",
    "    def postprocess(self, model, predictions, member_ids):\n",
    "        schedules = get_timevar_from_pyomo(model, predictions.index)\n",
    "        schedules[\"schedules\"] = (\n",
    "            schedules[\"batteryin\"] - schedules[\"batteryout\"]\n",
    "        ).copy()\n",
    "\n",
    "        if self.only_return_schedules:\n",
    "            schedules = schedules[\"schedules\"]\n",
    "            schedules.columns = member_ids\n",
    "        return schedules\n",
    "\n",
    "def get_timevar_from_pyomo(model, timeindex):\n",
    "    results = {}\n",
    "    for var in model.component_objects(Var):\n",
    "        if var.index_set()._implicit_subsets is None:\n",
    "            results[var.name] = pd.DataFrame(\n",
    "                index=[var.name], data=getattr(model, var.name).get_values()\n",
    "            ).transpose()\n",
    "            results[var.name].index = timeindex\n",
    "        else:\n",
    "            results[var.name] = (\n",
    "                pd.DataFrame(index=[\"none\"], data=getattr(model, var.name).get_values())\n",
    "                .transpose()\n",
    "                .unstack(level=1)\n",
    "            )\n",
    "            results[var.name].columns = results[var.name].columns.levels[1]\n",
    "            results[var.name].index = timeindex\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedules_concat = {}\n",
    "energy_concat = {}\n",
    "\n",
    "community_sizes = list(community_netload.keys())\n",
    "\n",
    "freq = \"2D\"\n",
    "freq_delta = timedelta(days=2) - deltat\n",
    "aging_cost = 1e-6\n",
    "battery_size = 5\n",
    "\n",
    "for community_size in tqdm(community_sizes, desc=\"Community size\"):\n",
    "    schedules_concat[community_size] = []\n",
    "    energy_concat[community_size] = []   \n",
    "    sub_house_ids = list(community_netload[community_size].columns)\n",
    "    \n",
    "    ref_spec = {\n",
    "            \"min_kw\": battery_size / 2,\n",
    "            \"max_kw\": battery_size / 2,\n",
    "            \"max_kwh\": battery_size,\n",
    "            \"init_kwh\": battery_size * 0.01,\n",
    "            \"eta\": 0.95,\n",
    "            \"offset\": 0.01}\n",
    "    spec = [dict(ref_spec) for i in sub_house_ids]\n",
    "    \n",
    "    \n",
    "    model = OptimizeBatteries_quadratic(\n",
    "        grid_buy=0.1740, grid_fee=0.0371, battery_aging_cost=aging_cost,\n",
    "        verbose=False, only_return_schedules=False)\n",
    "    \n",
    "    date_range = pd.date_range(end_training + deltat, end_complete_pred, freq=freq)\n",
    "    nb_iteration = len(date_range)\n",
    "    for jndex, opti_start in enumerate(date_range):\n",
    "        \n",
    "        end_optimization = opti_start + horizon + deltat\n",
    "        if end_optimization > end_complete_pred:\n",
    "            end_optimization = end_complete_pred\n",
    "        \n",
    "        tmp_netload = community_netload[community_size].loc[opti_start:end_optimization, :]\n",
    "        schedule = model.solve(tmp_netload, sub_house_ids, spec)\n",
    "        \n",
    "        # If not last iteration\n",
    "        if not jndex == nb_iteration - 1:\n",
    "            battery_time = opti_start + freq_delta + deltat\n",
    "            for i, house_id in enumerate(sub_house_ids):\n",
    "                spec[i][\"init_kwh\"] = float(schedule[\"batteryenergy\"].at[battery_time, i])\n",
    "\n",
    "        schedules_concat[community_size].append(schedule[\"schedules\"].loc[opti_start:opti_start+freq_delta].copy())\n",
    "        energy_concat[community_size].append(schedule[\"batteryenergy\"].loc[opti_start:opti_start+freq_delta].copy())\n",
    "    schedules_concat[community_size] = pd.concat(schedules_concat[community_size], axis=0)\n",
    "    energy_concat[community_size] = pd.concat(energy_concat[community_size], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perfect optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_schedules_concat = {}\n",
    "perfect_energy_concat = {}\n",
    "\n",
    "for community_size in tqdm(community_sizes, desc=\"Community size\"):\n",
    "    perfect_schedules_concat[community_size] = []\n",
    "    perfect_energy_concat[community_size] = []\n",
    "    sub_house_ids = list(community_netload[community_size].columns)\n",
    "    \n",
    "    ref_spec = {\n",
    "            \"min_kw\": battery_size / 2,\n",
    "            \"max_kw\": battery_size / 2,\n",
    "            \"max_kwh\": battery_size,\n",
    "            \"init_kwh\": battery_size * 0.01,\n",
    "            \"eta\": 0.95,\n",
    "            \"offset\": 0.01}\n",
    "    spec = [dict(ref_spec) for i in sub_house_ids]\n",
    "    \n",
    "    \n",
    "    model = OptimizeBatteries_quadratic(\n",
    "        grid_buy=0.1740, grid_fee=0.0371, battery_aging_cost=0,\n",
    "        verbose=False, only_return_schedules=False)\n",
    "    \n",
    "    date_range = pd.date_range(end_training + deltat, end_complete_pred, freq=freq)\n",
    "    nb_iteration = len(date_range)\n",
    "    for jndex, opti_start in enumerate(date_range):\n",
    "        \n",
    "        end_optimization = opti_start + horizon + deltat\n",
    "        if end_optimization > end_complete_pred:\n",
    "            end_optimization = end_complete_pred\n",
    "        \n",
    "        tmp_netload = individuals.loc[opti_start:end_optimization, sub_house_ids]\n",
    "        schedule = model.solve(tmp_netload, sub_house_ids, spec)\n",
    "        \n",
    "        # If not last iteration\n",
    "        if not jndex == nb_iteration - 1:\n",
    "            battery_time = opti_start + freq_delta + deltat\n",
    "            for i, house_id in enumerate(sub_house_ids):\n",
    "                spec[i][\"init_kwh\"] = float(schedule[\"batteryenergy\"].at[battery_time, i])\n",
    "\n",
    "        perfect_schedules_concat[community_size].append(schedule[\"schedules\"].loc[opti_start:opti_start+freq_delta].copy())\n",
    "        perfect_energy_concat[community_size].append(schedule[\"batteryenergy\"].loc[opti_start:opti_start+freq_delta].copy())\n",
    "    perfect_schedules_concat[community_size] = pd.concat(perfect_schedules_concat[community_size], axis=0)\n",
    "    perfect_energy_concat[community_size] = pd.concat(perfect_energy_concat[community_size], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare in €"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_start = end_training + deltat\n",
    "_end = end_complete_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_2d = []\n",
    "\n",
    "for community_size in community_sizes:\n",
    "    sub_house_ids = list(community_netload[community_size].columns)\n",
    "    graph = schedules_concat[community_size].copy()\n",
    "    graph.columns = sub_house_ids\n",
    "    for col in graph.columns:\n",
    "        graph[col] += individuals.loc[_start:_end, col]\n",
    "\n",
    "    forecast_cost = community_bill(graph, sub_house_ids, timing=\"15T\", deltat=15, buy_price=0.1740, exchange_price=0.0371)\n",
    "    #print(f\"Forecast for {community_size} houses, cost = {np.round(forecast_cost / community_size, 2)} €/household\")\n",
    "    cost_2d.append(forecast_cost / community_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_perfect = []\n",
    "\n",
    "for community_size in community_sizes:\n",
    "    sub_house_ids = list(community_netload[community_size].columns)\n",
    "    graph = perfect_schedules_concat[community_size].copy()\n",
    "    graph.columns = sub_house_ids\n",
    "    for col in graph.columns:\n",
    "        graph[col] += individuals.loc[_start:_end, col]\n",
    "\n",
    "    forecast_cost = community_bill(graph, sub_house_ids, timing=\"15T\", deltat=15, buy_price=0.1740, exchange_price=0.0371)\n",
    "    #print(f\"Forecast for {community_size} houses, cost = {np.round(forecast_cost / community_size, 2)} €/household\")\n",
    "    cost_perfect.append(forecast_cost / community_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_pvonly = []\n",
    "\n",
    "for community_size in community_sizes:\n",
    "    #print(f\"Size = {community_size}\")\n",
    "    sub_house_ids = list(community_netload[community_size].columns)\n",
    "    graph = individuals.loc[_start:_end, sub_house_ids].copy()\n",
    "\n",
    "    forecast_cost = community_bill(graph, sub_house_ids, timing=\"15T\", deltat=15, buy_price=0.1740, exchange_price=0.0371)\n",
    "    #print(f\"PV only cost = {np.round(forecast_cost / community_size, 2)} €/household\")\n",
    "    cost_pvonly.append(forecast_cost / community_size)\n",
    "\n",
    "    forecast_cost = community_bill(graph, sub_house_ids, timing=\"1D\", deltat=15, buy_price=0.1740, exchange_price=0.0371)\n",
    "    #print(f\"Theoretical 1d cost = {np.round(forecast_cost / community_size, 2)} €/household\")\n",
    "\n",
    "    forecast_cost = community_bill(graph, sub_house_ids, timing=\"2D\", deltat=15, buy_price=0.1740, exchange_price=0.0371)\n",
    "    #print(f\"Theoretical 2d cost = {np.round(forecast_cost / community_size, 2)} €/household\")\n",
    "\n",
    "    forecast_cost = community_bill(graph, sub_house_ids, timing=\"365D\", deltat=15, buy_price=0.1740, exchange_price=0.0371)\n",
    "    #print(f\"Theoretical cost = {np.round(forecast_cost / community_size, 2)} €/household\")\n",
    "    #print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph = pd.DataFrame(index=community_sizes, data={\"2D\": cost_2d,\n",
    "#                                                  \"Perfect\": cost_perfect,\n",
    "#                                                  \"PV_only\": cost_pvonly})\n",
    "\n",
    "with open(f'results/_graph_community_size.pickle', 'rb') as handle:\n",
    "    graph = pickle.load(handle)\n",
    "\n",
    "graph = graph[graph[\"PV_only\"] < 13]\n",
    "\n",
    "fig = px.line(graph)\n",
    "\n",
    "fig.update_traces(line_width=2)\n",
    "fig.data[-1].update(fill=\"tonexty\", fillcolor=\"rgba(214, 39, 40, 0.05)\")\n",
    "fig.data[1].update(fill=\"tonexty\", fillcolor=\"rgba(55, 128, 191, 0.2)\")\n",
    "fig.update_layout(\n",
    "    layout,\n",
    "    height=500,\n",
    "    width=800,\n",
    "    yaxis_title=\"Cost [€/member]\",\n",
    "    xaxis_title=\"Community sizes [kWh]\",\n",
    "    yaxis_range=[0, 13.2],\n",
    "    xaxis_range=[0, 120],\n",
    "    yaxis_dtick=1,\n",
    "    yaxis_showline=False, yaxis_linewidth=2, yaxis_linecolor='black',\n",
    "    xaxis_showline=False, xaxis_linewidth=2, xaxis_linecolor='black',\n",
    "    showlegend=False)\n",
    "fig.show()\n",
    "fig.write_image(f\"results/cost_community_size.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./results/cost_community_size.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results/_graph_community_size.pickle', 'wb') as handle:\n",
    "    pickle.dump(graph, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line((graph[\"2D\"] - graph[\"Perfect\"]) * 100 / (graph[\"PV_only\"] - graph[\"Perfect\"]))\n",
    "fig.update_traces(line_width=3)\n",
    "fig.update_layout(\n",
    "    layout,\n",
    "    height=500,\n",
    "    width=800,\n",
    "    yaxis_title=\"Cost gap [%]\",\n",
    "    xaxis_title=\"Community sizes [kWh]\",\n",
    "    yaxis_range=[0, 20],\n",
    "    xaxis_range=[0, 120],\n",
    "    yaxis_dtick=2,\n",
    "    yaxis_showline=False, yaxis_linewidth=2, yaxis_linecolor='black',\n",
    "    xaxis_showline=False, xaxis_linewidth=2, xaxis_linecolor='black',\n",
    "    showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./results/newplot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jonathan",
   "language": "python",
   "name": "jonathan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
